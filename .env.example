# ========================================
# CV-Enhancer Environment Configuration
# ========================================
# Copy this file to .env or .env.local for local development

# ----------------
# Mode Configuration
# ----------------
MODE=local

# ----------------
# LLM Configuration
# ----------------
LLM_PROVIDER=ollama  # Options: ollama, openai, anthropic, gemini
LLM_MODEL=llama3:8b  # Model name for the selected provider
LLM_API_KEY=         # Required for openai, anthropic (not needed for ollama, gemini)
LLM_API_BASE=http://localhost:11434  # For Ollama, set the API base URL
LLM_TEMPERATURE=0.7  # Temperature for LLM responses (0.0-1.0)
LLM_MAX_TOKENS=2048  # Maximum tokens for LLM responses

# ----------------
# Storage Configuration
# ----------------
STORAGE_TYPE=local
DATA_DIR=./data

# ----------------
# Vector Database Configuration
# ----------------
VECTOR_DB_TYPE=faiss
VECTOR_DB_PATH=./data/embeddings

# ----------------
# ADK Configuration
# ----------------
ADK_MODE=local
ADK_HOST=localhost
ADK_PORT=8000

# ----------------
# User Interaction
# ----------------
USER_INTERACTION_MODE=interactive  # Options: interactive, non-interactive
# - interactive: Prompt for user input via terminal
# - non-interactive: Use LLM inference only (for CI/CD)

# ----------------
# Logging
# ----------------
LOG_LEVEL=INFO       # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
